# titanic_boosted (beta)


We needed to classify passengers in a binary outcome (dead or survived)
At the beginning, I used only the sex and passenger class categories or this classification problem,. The age category would have been nice to use, but too much data was missing. I trained a couple different models (Naive Bayes, KNN, Random Forest) on the training set and applied k-fold cross validation to check the mean and variance that the models produced. Accuracy for the best ones seems to converge around 78%. I also tried using an ANN algorithm, but it also seemed to converge at a 78% accuracy. I finally choose to use the K Nearest Neighbors classifier because it would give a slightly lower variance than the other classification models. 
Trying to improve the model, I did more research on exploratory analysis (EAD), classifier models & how to optimally boost the parameters.
EAD : I started by displaying some graphs using seaborn and matplotlib to have some idea of what the data looked like, what I could do with it. I included more variables such as the age & embarked in the classifier. As only 2 values on 418 were missing in the embarked column of the training set, I replace those by the mode. As for the age class, the missing values were taken care of by the xgb classifier. If another classifier had been used, missing age values could have been replaced by the mean of the age given the person was traveling in class 1,2 or 3 (PClass category) and not by the global mean, since we saw in EAD, passengers in higher classes were of older age. Then I finished the preprocessing of the data by encoding the categorical data. Feature scaling did not need to be applied thanks to the nature of the XGB Classifier. If a model not tree based had been used like KMeans, we would have had to scale the features, with a Standard Scaler for instance.
I found that the XGB Classifier combined with thorough parameter tuning via GridSearchCV gave me the highest probability, around 86%. 
Some limitations of the subject: feature engineering the cabin column data. There were a lot more missing values than present which would have been time consuming. In the future, on other classification problems, I will document how to apply feature engineering for a feature significantly missing data. We could also try encoding the passenger class to see if it would yield a different result. In addition, not all paramaters of the XGB classifier were used, we could dig into finding the optimal value of those to increase the accuracy of our model.
